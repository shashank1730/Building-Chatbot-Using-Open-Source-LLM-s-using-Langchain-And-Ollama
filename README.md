# 🌟 LangChain + Ollama Series: Building Chatbots, APIs & RAG Pipelines

This repository is a personal learning journey exploring the power of **LangChain** and **open-source LLMs via Ollama**. Each project demonstrates a different real-world use case—from basic prompt interactions to FastAPI integrations and full Retrieval-Augmented Generation (RAG) pipelines.

---

## 📁 Project Structure

---

## 📦 Project 1: Search Box Chatbot (Customer Support Style)

**Summary:**  
A basic chatbot built using LangChain and Ollama where the user enters a query, and the LLM responds as a simulated customer support agent.

**Highlights:**

- PromptTemplate with role-based instructions
- User input handled via a search box
- LLM response streamed back for realistic interaction

---

## ⚡ Project 2: FastAPI + Ollama for Essay & Poem Generation

**Summary:**  
A FastAPI application that generates **essays or poems** based on the topic provided by the user.

**Endpoints:**

- `POST /essay` → Returns an essay on the given topic
- `POST /poem` → Returns a poem on the given topic

**Technologies:**

- LangChain + Ollama
- FastAPI
- JSON-based I/O for easy frontend or tool integration

---

## 🔍 Project 3: Retrieval-Augmented Generation (RAG) Pipeline

**Summary:**  
An end-to-end pipeline using LangChain to implement Retrieval-Augmented Generation with Ollama.

**Pipeline Flow:**

1. **Load** → Ingest documents
2. **Transform** → Chunk and clean text
3. **Embed** → Generate embeddings
4. **Store** → Save to a VectorDB (e.g., FAISS or Chroma)
5. **Query** → Accept user question
6. **Retrieve** → Fetch relevant documents
7. **Generate** → Use Ollama to respond with context

**Key Concepts:**

- LangChain Document Loaders, Text Splitters
- Embedding and Vector Stores
- Semantic Search + Context-Aware Generation

---

---

## 🧠 Project 4: Advanced RAG Chatbot using LangChain Chains (LLM-Driven)

**Summary:**  
This project demonstrates a **real Retrieval-Augmented Generation (RAG) pipeline** where the response is generated by an **LLM using context retrieved from a Vector DB**. Unlike basic RAG, here LangChain chains (`ConversationalRetrievalChain`) are used to fuse retrieval + LLM reasoning, enabling structured, contextual replies to user questions over PDFs.

**Pipeline Flow:**

1. **Load** → PDFs are loaded using LangChain's `PyPDFLoader`
2. **Chunk** → Text is split using `RecursiveCharacterTextSplitter`
3. **Embed** → OpenAI embeddings are generated and stored in **FAISS**
4. **Retrieve** → FAISS vector store is queried for relevant chunks
5. **Chain** → `create_retrieval_chain` sends results
6. **Generate** → LLM returns coherent, structured answers

**Key Concepts:**

- `ConversationalRetrievalChain` combines retrieval + prompting + memory
- LLM is explicitly used to **reason over context**, not just return matched chunks
- Custom system prompts can guide the LLM to act like a domain-specific expert
- `.env` used to manage OpenAI API keys securely

---

### 🔍 How It's Different from Project 3

| Feature                   | Project 3: Basic RAG                   | Project 4: Advanced RAG (LLM-Driven)               |
| ------------------------- | -------------------------------------- | -------------------------------------------------- |
| **LLM Involvement**       | ❌ No LLM used                         | ✅ LLM (via LangChain chain) generates output      |
| **Output Style**          | Raw matched text chunks from Vector DB | Clean, structured, human-like LLM response         |
| **Prompt Engineering**    | ❌ None                                | ✅ Custom prompts used inside LangChain chain      |
| **Conversation Handling** | ❌ No memory                           | ❌ No memory                                       |
| **LangChain Components**  | Loader, Splitter, Embedding, FAISS     | Loader, Splitter, FAISS + `create_retrieval_chain` |
| **Use Case Quality**      | Basic Q&A over documents               | Production-style intelligent chatbot over docs     |

---

**Tech Stack:**

- LangChain
- OpenAI Embeddings + LLM
- FAISS
- Python
- No UI or API layer (pure backend logic)

---

## 🤖 Project 4: Multi-Tool AI Agent (No OpenAI) for Policy & FAQ Understanding

**Summary:**  
This project implements a **tool-using AI Agent** built using **LangChain and Ollama**, with **zero dependency on OpenAI APIs**. The agent mimics decision-making based on the user's query and dynamically chooses the most appropriate knowledge source (tool), such as:

- 📚 **Wikipedia** → For general knowledge & background questions
- ✈️ **American Airlines Policy Parser** → For travel-specific queries (e.g., baggage, refunds, check-in rules)

**How It Works:**

1. **User Prompt** → A user submits a natural language query (e.g., "Can I check in my bags 2 days before the flight?")
2. **Tool Selection** → The LLM determines if the question is about:
   - General facts → Use **Wikipedia Tool**
   - Airline policy → Use **AA Policy Tool**
3. **Data Retrieval** → Fetches relevant info via scraping or preloaded sources
4. **Answer Generation** → LLM processes retrieved content and responds as a human-like assistant

**Key Features:**

- ✅ No OpenAI API required — uses **open-source LLMs** like `llama3` or `mistral` with **Ollama**
- 🧠 Agent "thinks" which source/tool to invoke — simulates reasoning & decision-making
- 🧰 Modular tool architecture — easily extend with new tools (e.g., Hotels.com, GitHub docs, etc.)
- 🔒 Fully local and private — no cloud dependency

---

### 🧪 Example Queries

- "Can I check in baggage two days in advance?" → AA Policy Tool
- "What is jet lag?" → Wikipedia Tool
- "How does American Airlines handle missed connections?" → AA Policy Tool
- "Where is the headquarters of American Airlines?" → Wikipedia Tool

---

**Tech Stack:**

- LangChain (Tool-using Agent setup)
- Ollama (local LLM runtime)
- FAISS (for retrieval if needed)
- BeautifulSoup or Playwright (for scraping)
- Python

---

### 🧠 How This Is Different from Project 3

| Feature              | Project 3: Basic RAG   | Project 4: Multi-Tool AI Agent                           |
| -------------------- | ---------------------- | -------------------------------------------------------- |
| **LLM Type**         | Can use OpenAI         | ✅ Purely open-source (Ollama + local models)            |
| **Tool Use**         | ❌ None                | ✅ Dynamic tool use based on query type                  |
| **Knowledge Source** | Single document corpus | Multiple tools: Wikipedia, AA Policies                   |
| **Routing Logic**    | ❌ None                | ✅ Agent decides source (tool) via LangChain agent logic |
| **Scalability**      | Focused on one domain  | Expandable to many domains (travel, support, etc.)       |

---

### 🌐 Future Plans

- Add more tools (e.g., GitHub Docs, News, Airline comparison)
- Add support for conversational memory
- UI via Streamlit or FastAPI frontend

## 🛠️ Setup Instructions

### Prerequisites:

- Python 3.9+
- [Ollama installed](https://ollama.com/)
- Recommended models: `llama3`, `mistral`, etc.

### Install Dependencies:

Navigate to each folder and run:

```bash
pip install -r requirements.txt
```

### 🙌 Why This Series?

This repo helps solidify concepts around:

- LangChain workflows and modules
- Prompt engineering
- API-based LLM services
- Building scalable RAG systems using **open-source LLMs** (no paid APIs!)

---

## 🚀 Future Additions

- Frontend integration with Streamlit or Next.js
- File/document upload support for RAG
- Chat history & memory in chatbot
- Evaluation of LLM outputs using LangChain tools

---

## 📬 Feedback

Feel free to open issues or discussions if you're on a similar journey or have suggestions!
