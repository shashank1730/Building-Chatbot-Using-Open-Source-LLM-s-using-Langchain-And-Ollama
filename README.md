# ğŸŒŸ LangChain + Ollama Series: Building Chatbots, APIs & RAG Pipelines

This repository is a personal learning journey exploring the power of **LangChain** and **open-source LLMs via Ollama**. Each project demonstrates a different real-world use caseâ€”from basic prompt interactions to FastAPI integrations and full Retrieval-Augmented Generation (RAG) pipelines.

---

## ğŸ“ Project Structure

---

## ğŸ“¦ Project 1: Search Box Chatbot (Customer Support Style)

**Summary:**  
A basic chatbot built using LangChain and Ollama where the user enters a query, and the LLM responds as a simulated customer support agent.

**Highlights:**

- PromptTemplate with role-based instructions
- User input handled via a search box
- LLM response streamed back for realistic interaction

---

## âš¡ Project 2: FastAPI + Ollama for Essay & Poem Generation

**Summary:**  
A FastAPI application that generates **essays or poems** based on the topic provided by the user.

**Endpoints:**

- `POST /essay` â†’ Returns an essay on the given topic
- `POST /poem` â†’ Returns a poem on the given topic

**Technologies:**

- LangChain + Ollama
- FastAPI
- JSON-based I/O for easy frontend or tool integration

---

## ğŸ” Project 3: Retrieval-Augmented Generation (RAG) Pipeline

**Summary:**  
An end-to-end pipeline using LangChain to implement Retrieval-Augmented Generation with Ollama.

**Pipeline Flow:**

1. **Load** â†’ Ingest documents
2. **Transform** â†’ Chunk and clean text
3. **Embed** â†’ Generate embeddings
4. **Store** â†’ Save to a VectorDB (e.g., FAISS or Chroma)
5. **Query** â†’ Accept user question
6. **Retrieve** â†’ Fetch relevant documents
7. **Generate** â†’ Use Ollama to respond with context

**Key Concepts:**

- LangChain Document Loaders, Text Splitters
- Embedding and Vector Stores
- Semantic Search + Context-Aware Generation

---

---

## ğŸ§  Project 4: Advanced RAG Chatbot using LangChain Chains (LLM-Driven)

**Summary:**  
This project demonstrates a **real Retrieval-Augmented Generation (RAG) pipeline** where the response is generated by an **LLM using context retrieved from a Vector DB**. Unlike basic RAG, here LangChain chains (`ConversationalRetrievalChain`) are used to fuse retrieval + LLM reasoning, enabling structured, contextual replies to user questions over PDFs.

**Pipeline Flow:**

1. **Load** â†’ PDFs are loaded using LangChain's `PyPDFLoader`
2. **Chunk** â†’ Text is split using `RecursiveCharacterTextSplitter`
3. **Embed** â†’ OpenAI embeddings are generated and stored in **FAISS**
4. **Retrieve** â†’ FAISS vector store is queried for relevant chunks
5. **Chain** â†’ `create_retrieval_chain` sends results
6. **Generate** â†’ LLM returns coherent, structured answers

**Key Concepts:**

- `ConversationalRetrievalChain` combines retrieval + prompting + memory
- LLM is explicitly used to **reason over context**, not just return matched chunks
- Custom system prompts can guide the LLM to act like a domain-specific expert
- `.env` used to manage OpenAI API keys securely

---

### ğŸ” How It's Different from Project 3

| Feature                   | Project 3: Basic RAG                   | Project 4: Advanced RAG (LLM-Driven)               |
| ------------------------- | -------------------------------------- | -------------------------------------------------- |
| **LLM Involvement**       | âŒ No LLM used                         | âœ… LLM (via LangChain chain) generates output      |
| **Output Style**          | Raw matched text chunks from Vector DB | Clean, structured, human-like LLM response         |
| **Prompt Engineering**    | âŒ None                                | âœ… Custom prompts used inside LangChain chain      |
| **Conversation Handling** | âŒ No memory                           | âŒ No memory                                       |
| **LangChain Components**  | Loader, Splitter, Embedding, FAISS     | Loader, Splitter, FAISS + `create_retrieval_chain` |
| **Use Case Quality**      | Basic Q&A over documents               | Production-style intelligent chatbot over docs     |

---

**Tech Stack:**

- LangChain
- OpenAI Embeddings + LLM
- FAISS
- Python
- No UI or API layer (pure backend logic)

---

## ğŸ¤– Project 4: Multi-Tool AI Agent (No OpenAI) for Policy & FAQ Understanding

**Summary:**  
This project implements a **tool-using AI Agent** built using **LangChain and Ollama**, with **zero dependency on OpenAI APIs**. The agent mimics decision-making based on the user's query and dynamically chooses the most appropriate knowledge source (tool), such as:

- ğŸ“š **Wikipedia** â†’ For general knowledge & background questions
- âœˆï¸ **American Airlines Policy Parser** â†’ For travel-specific queries (e.g., baggage, refunds, check-in rules)

**How It Works:**

1. **User Prompt** â†’ A user submits a natural language query (e.g., "Can I check in my bags 2 days before the flight?")
2. **Tool Selection** â†’ The LLM determines if the question is about:
   - General facts â†’ Use **Wikipedia Tool**
   - Airline policy â†’ Use **AA Policy Tool**
3. **Data Retrieval** â†’ Fetches relevant info via scraping or preloaded sources
4. **Answer Generation** â†’ LLM processes retrieved content and responds as a human-like assistant

**Key Features:**

- âœ… No OpenAI API required â€” uses **open-source LLMs** like `llama3` or `mistral` with **Ollama**
- ğŸ§  Agent "thinks" which source/tool to invoke â€” simulates reasoning & decision-making
- ğŸ§° Modular tool architecture â€” easily extend with new tools (e.g., Hotels.com, GitHub docs, etc.)
- ğŸ”’ Fully local and private â€” no cloud dependency

---

### ğŸ§ª Example Queries

- "Can I check in baggage two days in advance?" â†’ AA Policy Tool
- "What is jet lag?" â†’ Wikipedia Tool
- "How does American Airlines handle missed connections?" â†’ AA Policy Tool
- "Where is the headquarters of American Airlines?" â†’ Wikipedia Tool

---

**Tech Stack:**

- LangChain (Tool-using Agent setup)
- Ollama (local LLM runtime)
- FAISS (for retrieval if needed)
- BeautifulSoup or Playwright (for scraping)
- Python

---

### ğŸ§  How This Is Different from Project 3

| Feature              | Project 3: Basic RAG   | Project 4: Multi-Tool AI Agent                           |
| -------------------- | ---------------------- | -------------------------------------------------------- |
| **LLM Type**         | Can use OpenAI         | âœ… Purely open-source (Ollama + local models)            |
| **Tool Use**         | âŒ None                | âœ… Dynamic tool use based on query type                  |
| **Knowledge Source** | Single document corpus | Multiple tools: Wikipedia, AA Policies                   |
| **Routing Logic**    | âŒ None                | âœ… Agent decides source (tool) via LangChain agent logic |
| **Scalability**      | Focused on one domain  | Expandable to many domains (travel, support, etc.)       |

---

### ğŸŒ Future Plans

- Add more tools (e.g., GitHub Docs, News, Airline comparison)
- Add support for conversational memory
- UI via Streamlit or FastAPI frontend

## ğŸ› ï¸ Setup Instructions

### Prerequisites:

- Python 3.9+
- [Ollama installed](https://ollama.com/)
- Recommended models: `llama3`, `mistral`, etc.

### Install Dependencies:

Navigate to each folder and run:

```bash
pip install -r requirements.txt
```

### ğŸ™Œ Why This Series?

This repo helps solidify concepts around:

- LangChain workflows and modules
- Prompt engineering
- API-based LLM services
- Building scalable RAG systems using **open-source LLMs** (no paid APIs!)

---

## ğŸš€ Future Additions

- Frontend integration with Streamlit or Next.js
- File/document upload support for RAG
- Chat history & memory in chatbot
- Evaluation of LLM outputs using LangChain tools

---

## ğŸ“¬ Feedback

Feel free to open issues or discussions if you're on a similar journey or have suggestions!
