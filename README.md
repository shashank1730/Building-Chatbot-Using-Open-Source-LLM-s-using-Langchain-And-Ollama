# 🌟 LangChain + Ollama Series: Building Chatbots, APIs & RAG Pipelines

This repository is a personal learning journey exploring the power of **LangChain** and **open-source LLMs via Ollama**. Each project demonstrates a different real-world use case—from basic prompt interactions to FastAPI integrations and full Retrieval-Augmented Generation (RAG) pipelines.

---

## 📁 Project Structure

---

## 📦 Project 1: Search Box Chatbot (Customer Support Style)

**Summary:**  
A basic chatbot built using LangChain and Ollama where the user enters a query, and the LLM responds as a simulated customer support agent.

**Highlights:**

- PromptTemplate with role-based instructions
- User input handled via a search box
- LLM response streamed back for realistic interaction

---

## ⚡ Project 2: FastAPI + Ollama for Essay & Poem Generation

**Summary:**  
A FastAPI application that generates **essays or poems** based on the topic provided by the user.

**Endpoints:**

- `POST /essay` → Returns an essay on the given topic
- `POST /poem` → Returns a poem on the given topic

**Technologies:**

- LangChain + Ollama
- FastAPI
- JSON-based I/O for easy frontend or tool integration

---

## 🔍 Project 3: Retrieval-Augmented Generation (RAG) Pipeline

**Summary:**  
An end-to-end pipeline using LangChain to implement Retrieval-Augmented Generation with Ollama.

**Pipeline Flow:**

1. **Load** → Ingest documents
2. **Transform** → Chunk and clean text
3. **Embed** → Generate embeddings
4. **Store** → Save to a VectorDB (e.g., FAISS or Chroma)
5. **Query** → Accept user question
6. **Retrieve** → Fetch relevant documents
7. **Generate** → Use Ollama to respond with context

**Key Concepts:**

- LangChain Document Loaders, Text Splitters
- Embedding and Vector Stores
- Semantic Search + Context-Aware Generation

---

---

## 🧠 Project 4: Advanced RAG Chatbot using LangChain Chains (LLM-Driven)

**Summary:**  
This project demonstrates a **real Retrieval-Augmented Generation (RAG) pipeline** where the response is generated by an **LLM using context retrieved from a Vector DB**. Unlike basic RAG, here LangChain chains (`ConversationalRetrievalChain`) are used to fuse retrieval + LLM reasoning, enabling structured, contextual replies to user questions over PDFs.

**Pipeline Flow:**

1. **Load** → PDFs are loaded using LangChain's `PyPDFLoader`
2. **Chunk** → Text is split using `RecursiveCharacterTextSplitter`
3. **Embed** → OpenAI embeddings are generated and stored in **FAISS**
4. **Retrieve** → FAISS vector store is queried for relevant chunks
5. **Chain** → `create_retrieval_chain` sends results 
6. **Generate** → LLM returns coherent, structured answers

**Key Concepts:**

- `ConversationalRetrievalChain` combines retrieval + prompting + memory
- LLM is explicitly used to **reason over context**, not just return matched chunks
- Custom system prompts can guide the LLM to act like a domain-specific expert
- `.env` used to manage OpenAI API keys securely

---

### 🔍 How It's Different from Project 3

| Feature                   | Project 3: Basic RAG                   | Project 4: Advanced RAG (LLM-Driven)               |
| ------------------------- | -------------------------------------- | -------------------------------------------------- |
| **LLM Involvement**       | ❌ No LLM used                         | ✅ LLM (via LangChain chain) generates output      |
| **Output Style**          | Raw matched text chunks from Vector DB | Clean, structured, human-like LLM response         |
| **Prompt Engineering**    | ❌ None                                | ✅ Custom prompts used inside LangChain chain      |
| **Conversation Handling** | ❌ No memory                           | ❌ No memory                                       |
| **LangChain Components**  | Loader, Splitter, Embedding, FAISS     | Loader, Splitter, FAISS + `create_retrieval_chain` |
| **Use Case Quality**      | Basic Q&A over documents               | Production-style intelligent chatbot over docs     |

---

**Tech Stack:**

- LangChain
- OpenAI Embeddings + LLM
- FAISS
- Python
- No UI or API layer (pure backend logic)

---

## 🛠️ Setup Instructions

### Prerequisites:

- Python 3.9+
- [Ollama installed](https://ollama.com/)
- Recommended models: `llama3`, `mistral`, etc.

### Install Dependencies:

Navigate to each folder and run:

```bash
pip install -r requirements.txt
```

### 🙌 Why This Series?

This repo helps solidify concepts around:

- LangChain workflows and modules
- Prompt engineering
- API-based LLM services
- Building scalable RAG systems using **open-source LLMs** (no paid APIs!)

---

## 🚀 Future Additions

- Frontend integration with Streamlit or Next.js
- File/document upload support for RAG
- Chat history & memory in chatbot
- Evaluation of LLM outputs using LangChain tools

---

## 📬 Feedback

Feel free to open issues or discussions if you're on a similar journey or have suggestions!
